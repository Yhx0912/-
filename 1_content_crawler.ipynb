{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上证50 ——zssh000016——page544——content40892  √\n",
    "* 沪深300——zssh000300——page420——content32672   √\n",
    "* 中证1000——zssh000852——page41——content2589   √\n",
    "* 中证500——zssh000905——page68——content4774  √\n",
    "* 上证指数——zssh000001——page71370——content5617203\n",
    "* 深证成指——zssz399001——page5740——content447793  √\n",
    "\n",
    "\n",
    "* 上交所50ETF——sh510050——804——55591\n",
    "* 沪深300ETF——sh510300——374——24403\n",
    "* 中证500ETF——sh510500——106——7156\n",
    "* 科创50ETF——sh588000——298——22449\n",
    "* 豆粕ETF——sz159985——40——2766\n",
    "* 黄金ETF——sh518880——57——4222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd        #保存文件\n",
    "import numpy as np\n",
    "import requests            #获取网页\n",
    "from lxml import etree     #解析文档\n",
    "from faker import Factory# 生成不同的user-agent  ## 可能需要 pip install faker\n",
    "import time\n",
    "import random \n",
    "from multiprocessing.dummy import Pool as ThreadPool  # 线程池\n",
    "import re\n",
    "#from Crawler import crawler_url_list ## 封装函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = \"sh518880\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取数据\n",
    "#url_list = pd.read_csv(f\"url_list_{stock}.csv\",header=None)\n",
    "url_list = pd.read_csv(f\"url_list_{stock}_ETF.csv\",header=None)\n",
    "url_list = url_list.iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = int(input(\"需要处理的block：\"))  ## 目前在 【 0 】 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5百多万数据过多，计划每50万为一个Block。\n",
    "\"\"\"\n",
    "block_nums = 500000\n",
    "\n",
    "def get_block_list(url_list,block_nums, start=0):  ## 获取 分部分爬虫list\n",
    "    url_nums = len(url_list)\n",
    "    temp_index = np.arange(start,url_nums,block_nums).tolist()\n",
    "    temp_index.append(url_nums)\n",
    "    \n",
    "    part_block_list = []\n",
    "    for i in range(len(temp_index)-1):\n",
    "        part_block_list.append(url_list[temp_index[i]:temp_index[i+1]])\n",
    "    return part_block_list\n",
    "\n",
    "url_list_block = get_block_list(url_list,block_nums, start=0)\n",
    "print(\"总共有\",len(url_list_block),\"个block\")\n",
    "\n",
    "url_list = url_list_block[block]\n",
    "print(\"当前block个内有\",len(url_list_block),\"个url\")\n",
    "print(\"*******当前为第\",block+1,\"个block*******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取 IPpoor.text\n",
    "IP_pool = []\n",
    "file=open('IP_poor_0103.txt')\n",
    "for f in file.read().splitlines():\n",
    "    IP_pool.append(eval(f))\n",
    "print(len(IP_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_deal(temp_text):  ## 文本处理函数\n",
    "    str = ''\n",
    "    temp_text1 = str.join(temp_text).strip()  ## 合并分散的文本\n",
    "    temp_text1 = re.sub('[\\r\\n]', '', temp_text1).strip()\n",
    "    temp_text2 = re.sub('[\\xa0]', '', temp_text1).strip()  ## 清楚空格\n",
    "    return temp_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guba_content(url): ## 获取 url里内容和日期\n",
    "    ## 生成随机user-agent\n",
    "    fc = Factory.create() \n",
    "    header = {'User-Agent': fc.user_agent()}\n",
    "    ## 获取网页\n",
    "    response = requests.get(url = url,headers=header,proxies=random.choice(IP_pool)) \n",
    "    #print(\"正在爬取:\", url, \"状态:\",response)\n",
    "    if  response.status_code == 200:\n",
    "        ## 解析文档\n",
    "        root = etree.HTML(response.text) \n",
    "        ## 爬文本内容\n",
    "        temp_content = root.xpath(\"//div[@id='mainbody']//div[@id='zwconbody']//text()\")\n",
    "        content = text_deal(temp_content)\n",
    "        ## 爬文本时间\n",
    "        temp_time = root.xpath(\"//div[@id='mainbody']//div[@id='zwcontent']//div[@class='zwfbtime']/text()\")\n",
    "        date = text_deal(temp_time)\n",
    "    else:\n",
    "        #print(\"状态错误:\", response.status_code)\n",
    "        content = \"ERROR\"\n",
    "        date = \"ERROR\"\n",
    "\n",
    "    ## 设置睡眠，反爬虫\n",
    "    t = random.uniform(0, 1.5)\n",
    "    time.sleep(t)\n",
    "    return content, date, response.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_list(url_list,part_nums, start=0):  ## 获取 分部分爬虫list\n",
    "    url_nums = len(url_list)\n",
    "    temp_index = np.arange(start,url_nums,part_nums).tolist()\n",
    "    temp_index.append(url_nums)\n",
    "    \n",
    "    part_url_list = []\n",
    "    for i in range(len(temp_index)-1):\n",
    "        part_url_list.append(url_list[temp_index[i]:temp_index[i+1]])\n",
    "    return part_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_time = 6 ## 一个part 6秒\n",
    "part_nums = 10000   ## 一个part的数量\n",
    "threading_num = 200 ## 进程数量\n",
    "\n",
    "## 注： part_nums//threading_num < 99   【5000//60】？ 【10000//120】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_url_list = get_part_list(url_list,part_nums, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(part_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(part_url_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#存在错误 Invalid URL '\\x1a': No scheme supplied. Perhaps you meant http://\u001a? 时，用于检验与处理\n",
    "\"\"\"\n",
    "\n",
    "len(part_url_list[-1])\n",
    "for i in range(len(part_url_list[-1])):        \n",
    "    if part_url_list[-1][i][:8] == \"https://\":\n",
    "        pass\n",
    "    else:\n",
    "        print(i)\n",
    "del(part_url_list[-1][-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 生成容器\n",
    "all_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_part_i = int(input(\"选择运行part的起点：\"))   ## 目前在【 0 】    ## 选择运行的起点--针对报错停止后重新运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i,part_url in enumerate(part_url_list):\n",
    "    if i < run_part_i:   ## 错误位置重新运行\n",
    "        pass\n",
    "    else:\n",
    "        print(\"*******正在处理第 【\", i ,\"】 part*******\")\n",
    "        p = ThreadPool(threading_num)\n",
    "        content = p.map(get_guba_content,part_url)   ## [(content, data, stata)]\n",
    "\n",
    "        if content[-1][-1] == 200:\n",
    "            all_data += content\n",
    "        else:\n",
    "            print(\"??????错误在第 【\", i ,\"】 part???????\")\n",
    "            break\n",
    "\n",
    "        print(\"#######正在睡眠#######\")\n",
    "        time.sleep(sleep_time)\n",
    "end = time.time()\n",
    "\n",
    "print(\"运行时间：\",(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=all_data,columns=[\"content\",\"date\",\"stata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_part_i = int(input(\"保存运行part的终点：\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 保存 data_content_{stock}_{block}_{run_part_i}.csv\n",
    "#save_name = f\"D:/股票指数类/data/data_content_{stock}_{block}_{run_part_i}.csv\"  ## _{start_page}_{end_page}\n",
    "save_name = f\"D:/股票指数类/data/data_content_{stock}_ETF.csv\"  ## _{start_page}_{end_page}\n",
    "df.to_csv(save_name,header=0,index=False) # header=0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 每次保存完记清空内存"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
